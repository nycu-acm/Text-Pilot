<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <title>Text-Pilot: Intelligent Visual Text Planning and Manipulation via Multi-modal LLM as Agent</title>
  <meta name="description" content="Text-Pilot: Intelligent Visual Text Planning and Manipulation via Multi-modal LLM as Agent">
  
  <meta property="og:title" content="Text-Pilot: Intelligent Visual Text Planning and Manipulation via Multi-modal LLM as Agent">
  <meta property="og:description" content="Intelligent Visual Text Planning and Manipulation via Multi-modal LLM as Agent">

  <link rel="stylesheet" href="assets/css/styles.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700;800&display=swap" rel="stylesheet">
</head>

<body>
  <nav class="nav">
    <div class="container nav-inner">
      <div class="brand">Text-Pilot</div>
      <div class="links">
        <a class="btn" href="#abstract">Abstract</a>
        <a class="btn" href="#method">Method</a>
        <a class="btn" href="#results">Results</a>
        <a class="btn" href="#bibtex">BibTeX</a>
      </div>
    </div>
  </nav>

  <header class="hero">
    <div class="container">
      <div class="badge-row"><span class="badge">Text To Image Generation ‚Ä¢ Multi-modal Large Language Models ‚Ä¢ Visual Text Generation ‚Ä¢ Visual Text Editing ‚Ä¢ Agent System</span></div>

      <h1 class="title-main">Text-Pilot</h1>
      <h2 class="title-sub">Intelligent Visual Text Planning and Manipulation via Multi-modal LLM as Agent</h2>

      <div class="authors">
        <a target="_blank" rel="noopener">Yuan-Kang Kuo</a>,
        <a target="_blank" rel="noopener">Quang-Thang Le</a>,
        <a target="_blank" rel="noopener">Ching-Chun Huang</a>
      </div>

      <div class="affil">Department of Computer Science, National Yang Ming Chiao Tung University</div>

      <div class="logo-row">
        <img src="assets/img/logo_nycu.png" alt="NYCU Logo" style="height:100px; width:auto;">
        <img src="assets/img/logo_acm.png" alt="ACM Logo" style="height:100px; width:auto;">
      </div>

      <div class="cta-row">
        <a class="btn primary" href="#" aria-disabled="true" target="_blank" rel="noopener">Paper (coming soon)</a>
        <a class="btn primary" href="https://github.com/nycu-acm/Text-Pilot" target="_blank" rel="noopener">Code</a>
        <!-- <a class="btn primary" href="#" aria-disabled="true" target="_blank" rel="noopener">Video (coming soon)</a> -->
      </div>
    </div>
  </header>

  <main>
    <!-- ABSTRACT + CONTRIBUTION -->
    <section id="abstract" class="section">
      <div class="container">
        <h2>üìù Abstract</h2>
        <p>In recent years, text-to-image diffusion models have achieved remarkable progress in generating photorealistic and semantically coherent images. However, producing accurate and readable text within generated scenes remains a long-standing challenge‚Äîtypical failures include misspellings, missing characters, or semantic inconsistencies between the visual text and its prompt. Prior approaches either fine-tune diffusion architectures with additional datasets or rely on manual post-processing tools, yet they still lack an effective verification and self-correction mechanism.To address this, we propose Text-Pilot, a unified and training-free framework that leverages a Multimodal Large Language Model (MLLM) as an autonomous agent to reason about text‚Äìprompt inconsistencies and coordinate multiple visual foundation models for correction. The system performs iterative perception‚Äìreasoning‚Äìaction cycles, automatically detecting, editing, erasing, and regenerating textual regions to ensure consistency and legibility. We conduct extensive experiments and rating-based evaluations, incorporating several Vision-Language Models (VLMs) to validate its effectiveness. The results show that Text-Pilot can be seamlessly integrated with various T2I models, substantially improving textual accuracy while maintaining overall image quality.</p>
        
        <!-- <div class="row">
          <div class="col-12">
            <div class="card">
              <h3 id="contribution">LLM-based time series forecasting</h3>
              <div class="fig">
                <img src="assets/img/contribution.png" alt="LLM-based time series forecasting" 
                    style="display:block; margin:0 auto; max-width:70%; height:auto;">
              </div>
              <p class="caption">Comparison of architectural designs for LLM-based time series forecasting.</p>
            </div>
          </div>
        </div>
      </div>
    </section> -->

    <!-- METHOD -->
    <section id="method" class="section">
      <div class="container">
        <h2>üìò Methodology</h2>
        <p>We present an overview of Text-Pilot. The text spotting module provides the MLLM with the detected textual content, their corresponding coordinate information, and mask images.The MLLM agent is responsible for verification and planning throughout the entire system, including evaluating textual correctness, formulating correction plans, and progressively verifying the processes of image generation, editing, and self-correction. It then invokes specific tools from the Text Manipulation Tool Library to perform concrete operations
        <div class="row">
          <div class="col-12">
            <div class="card">
              <h3>üèóÔ∏è Text-Pilot Pipeline</h3>
              <div class="fig"><img src="assets/img/Figure 3. Overview.jpg" alt="Pipeline of Text-Pilot"></div>
              <p class="caption">System overview of Text-Pilot. The pipeline begins with a user text prompt, which is converted into an initial image by a T2I model. The Text Spotting module detects all textual re- gions and provides them, along with the image and prompt, to the MLLM Agent for verification and correction planning. The Text Manipulation modules then perform editing operations, The corrected image is subsequently fed back into the next iteration, forming a closed self-correction loop.</p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- RESULTS -->
    <section id="results" class="section">
      <div class="container">
        <h2>üß™ Experiments</h2>
        <div class="row">
          
          <div class="col-12">
            <div class="card">
              <h3>Quantitative Metrics</h3>
              <div class="fig"><img src="assets/img/results.png" alt="Quantitative results table"></div>
              <p class="caption">Quantitative comparison of different methods integrated with Text-Pilot across multiple evaluation metrics</p>
            </div>
          </div>
          
          <!-- <div class="col-12">
            <div class="card">
              <h3> The Effectiveness of LLMs in Time Series Forecasting</h3>
              <div class="fig"><img src="assets/img/tab_4.png" alt=" The Effectiveness of LLMs in Time Series Forecasting"></div>
              <p class="caption">Do pretrained language models contribute to forecasting performance?</p>
            </div>
          </div>
          
          <div class="col-12">
            <div class="card">
              <h3>Computational Efficiency Analysis</h3>
              <div class="fig"><img src="assets/img/computation_cost.png" alt="Computational Efficiency Analysis"></div>
              <p class="caption">Training and inference times per batch (in seconds) for LAMP and TEMPO models on the ETTh1 dataset.</p>
            </div>
          </div> -->


          <!-- <div class="col-12">
            <div class="card">
              <h3>Visual Results</h3>
              <div class="gallery one-col">
                <div class="fig"><img src="assets/img/Figure 7. Visualization GPT-1.jpg" alt="Visual results"><div class="caption">Forecasting visualization across four representative examples from the ETT, weather, traffic dataset. Each subplot shows the input historical values (blue), ground-truth future values (green), and predicted values (red dashed) by LAMP (left) and AutoTimes (right). The sequence length and prediction horizon are both set to 96. Results highlight the accuracy and adaptability of LAMP across diverse temporal patterns.</div></div>
              </div>
            </div>
          </div> -->
          <div class="col-12">
            <div class="card">
              <h3>Visual Results</h3>
          
              <div class="gallery one-col">
          
                <!-- ÂúñÁâá 1 -->
                <div class="fig">
                  <img src="assets/img/Figure 7. Visualization GPT-1.jpg" alt="Visual results 1">
                </div>
          
                <!-- ÂúñÁâá 2 -->
                <div class="fig">
                  <img src="assets/img/Figure 7. Visualization GPT-2.jpg" alt="Visual results 2">
                </div>
          
                <!-- ÂúñÁâá 3 -->
                <div class="fig">
                  <img src="assets/img/Figure 7. Visualization SD3-1.jpg" alt="Visual results 3">
                </div>
          
                <!-- ÊúÄÂæå‰∏ÄÂºµÂúñ + caption -->
                <div class="fig">
                  <img src="assets/img/Figure 7. Visualization SD3-2.jpg" alt="Visual results final">
                  <div class="caption">
                    Visualization results from Text-Pilot
                  </div>
                </div>
          
              </div>
            </div>
          </div>

          
        </div> <!-- row -->
      </div> <!-- container -->
    </section>

    <!-- ACKNOWLEDGMENT -->
    <section id="acknowledgment" class="section">
    <div class="container">
        <h2>üíå Acknowledgment</h2>
        <div class="card">
        <p style="font-size: 1.05rem; line-height: 1.7; margin: 0; text-align: justify; text-justify: inter-word;">
          I would like to express my sincere gratitude to Professor <strong>Ching-Chun Huang</strong>, who has always been willing to devote his valuable time to guide my research. From shaping my research direction, reviewing literature, to structuring and writing this thesis, Professor Huang patiently led me step by step. With his guidance, I grew from someone who initially had no clear sense of how to conduct research into someone capable of reading papers independently, thinking critically, and eventually developing a research direction of my own.
        </div>
    </div>
    </section>
  </main>

  <footer>
    <div class="container">
      <div>¬© 2025 LAMP. This website is adapted from <a href="https://nycu-acm.github.io/UIStyler/website/" target="_blank" rel="noopener">UIStyler</a></div>
    </div>
  </footer>

  <script src="assets/js/main.js"></script>
</body>
</html>
